{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg+qWPUEOnn9ZqAZjggch7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asiabak/repozytorium1/blob/main/SVM_on_clean_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DcR2-2pOwN4k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    # Remove @mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Handle hashtags (keep the text without #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    # Remove special characters and extra whitespace\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Normalize whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text.strip()\n",
        "\n",
        "def process_file(input_path, output_path):\n",
        "    # Read the file using pandas, specifying no header and column names\n",
        "    df = pd.read_csv(input_path,\n",
        "                     sep='\\t',\n",
        "                     header=None,\n",
        "                     names=['latitude', 'longitude', 'text'])\n",
        "\n",
        "    # Clean the text column\n",
        "    df['text'] = df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "    # Save to new file, preserving tab separation and removing index\n",
        "    df.to_csv(output_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "# Example usage\n",
        "input_file = \"test_blind.txt\"\n",
        "output_file = \"test_blind_clean.txt\"\n",
        "\n",
        "try:\n",
        "    process_file(input_file, output_file)\n",
        "    print(f\"Successfully cleaned text and saved to {output_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u37va33awnvh",
        "outputId": "ab2246e8-4292-44fe-a704-246dbf2a2be5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            latitude  longitude text\n",
            "0  Und epis gfangä? Wo bisch gsi? Schächä? Nid be...        NaN  nan\n",
            "1  Ich werde niemals Menschen verstehen, die eine...        NaN  nan\n",
            "2  Immer wieder luschtig Die post woni als modera...        NaN  nan\n",
            "3  Du denksch, din Job isch sinnlos? Es git lüüt ...        NaN  nan\n",
            "4  kennt öber e guete Tättovierer? I bi no am Üeb...        NaN  nan\n",
            "Successfully cleaned text and saved to test_blind_clean.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9sS_w1n1bzD",
        "outputId": "0ea6ba52-b622-49ab-90b1-2d1748d483c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Using cached fasttext-0.9.3.tar.gz (73 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313475 sha256=ba23b778aab5c4e8d486b5db957567e0380a11e7f39f3d5d5eaa782007e3f1d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# File paths\n",
        "TRAIN_FILE = 'train_clean.txt'\n",
        "DEV_FILE = 'dev_clean.txt'\n",
        "TEST_FILE = 'test_blind_clean.txt'\n",
        "\n",
        "# Download and load FastText aligned vectors for German\n",
        "# print(\"Downloading and loading FastText model...\")\n",
        "# fasttext.util.download_model('de', if_exists='ignore')  # Download aligned German vectors\n",
        "ft = fasttext.load_model('cc.de.300.bin')\n",
        "\n",
        "# Function to load training and development data (with labels)\n",
        "def load_labeled_data(file_path):\n",
        "    \"\"\"Load data with latitude, longitude, and text.\"\"\"\n",
        "    sentences, labels = [], []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 3:\n",
        "                try:\n",
        "                    lat, lon = float(parts[0]), float(parts[1])\n",
        "                    tweet = parts[2]\n",
        "                    sentences.append(tweet)\n",
        "                    labels.append((lat, lon))\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    X = np.array([ft.get_sentence_vector(tweet) for tweet in sentences])\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "# Function to load test data (without labels)\n",
        "def load_unlabeled_data(file_path):\n",
        "    \"\"\"Load data with only text (for predictions).\"\"\"\n",
        "    sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweet = line.strip()\n",
        "            if tweet:  # Ensure the line is not empty\n",
        "                sentences.append(tweet)\n",
        "    X = np.array([ft.get_sentence_vector(tweet) for tweet in sentences])\n",
        "    return X\n",
        "\n",
        "# Load training and development data\n",
        "print(\"Loading training and development data...\")\n",
        "X_train, y_train = load_labeled_data(TRAIN_FILE)\n",
        "X_dev, y_dev = load_labeled_data(DEV_FILE)\n",
        "\n",
        "# Train the SVM model\n",
        "print(\"Training SVM model...\")\n",
        "svm = MultiOutputRegressor(SVR(kernel='poly'))\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the development set\n",
        "print(\"Evaluating on development set...\")\n",
        "y_dev_pred = svm.predict(X_dev)\n",
        "mse = mean_squared_error(y_dev, y_dev_pred, multioutput='raw_values')\n",
        "print(\"Mean Squared Error for each label on dev set:\", mse)\n",
        "print(\"Average Mean Squared Error on dev set:\", np.mean(mse))\n",
        "\n",
        "# Load test data and make predictions\n",
        "print(\"Loading test data and predicting...\")\n",
        "X_test = load_unlabeled_data(TEST_FILE)\n",
        "predictions = svm.predict(X_test)\n",
        "\n",
        "# Save predictions to a file\n",
        "output_file = 'test_predictions.txt'\n",
        "print(f\"Saving predictions to {output_file}...\")\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for pred in predictions:\n",
        "        f.write(f\"{pred[0]}\\t{pred[1]}\\n\")\n",
        "print(\"Predictions saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQdOZfNm1R1B",
        "outputId": "63941673-ba5e-45da-b977-9d7da90501ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training and development data...\n",
            "Training SVM model...\n",
            "Evaluating on development set...\n",
            "Mean Squared Error for each label on dev set: [0.04692826 0.30103779]\n",
            "Average Mean Squared Error on dev set: 0.1739830209017215\n",
            "Loading test data and predicting...\n",
            "Saving predictions to test_predictions.txt...\n",
            "Predictions saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yNZ7hAGEBsx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}