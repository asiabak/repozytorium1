{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVSjwvase50cQgo2CGcGXe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asiabak/repozytorium1/blob/main/SVM_generated_by_GPT_fasttext_aligned_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdjJ6hB1WBDw",
        "outputId": "7eb719de-f9bd-40ce-dd4d-d3ace2e7474f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296228 sha256=943884b53722a32fa775bf65fce2afb47b647b22207c11698a66bf926fd84430\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ],
      "source": [
        "pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# File paths\n",
        "TRAIN_FILE = 'train.txt'\n",
        "DEV_FILE = 'dev.txt'\n",
        "TEST_FILE = 'test_blind.txt'\n",
        "\n",
        "# Download and load FastText aligned vectors for German\n",
        "print(\"Downloading and loading FastText model...\")\n",
        "fasttext.util.download_model('de', if_exists='ignore')  # Download aligned German vectors\n",
        "ft = fasttext.load_model('cc.de.300.bin')\n",
        "\n",
        "# Function to load training and development data (with labels)\n",
        "def load_labeled_data(file_path):\n",
        "    \"\"\"Load data with latitude, longitude, and text.\"\"\"\n",
        "    sentences, labels = [], []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 3:\n",
        "                try:\n",
        "                    lat, lon = float(parts[0]), float(parts[1])\n",
        "                    tweet = parts[2]\n",
        "                    sentences.append(tweet)\n",
        "                    labels.append((lat, lon))\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    X = np.array([ft.get_sentence_vector(tweet) for tweet in sentences])\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "# Function to load test data (without labels)\n",
        "def load_unlabeled_data(file_path):\n",
        "    \"\"\"Load data with only text (for predictions).\"\"\"\n",
        "    sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweet = line.strip()\n",
        "            if tweet:  # Ensure the line is not empty\n",
        "                sentences.append(tweet)\n",
        "    X = np.array([ft.get_sentence_vector(tweet) for tweet in sentences])\n",
        "    return X\n",
        "\n",
        "# Load training and development data\n",
        "print(\"Loading training and development data...\")\n",
        "X_train, y_train = load_labeled_data(TRAIN_FILE)\n",
        "X_dev, y_dev = load_labeled_data(DEV_FILE)\n",
        "\n",
        "# Train the SVM model\n",
        "print(\"Training SVM model...\")\n",
        "svm = MultiOutputRegressor(SVR(kernel='rbf'))\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the development set\n",
        "print(\"Evaluating on development set...\")\n",
        "y_dev_pred = svm.predict(X_dev)\n",
        "mse = mean_squared_error(y_dev, y_dev_pred, multioutput='raw_values')\n",
        "print(\"Mean Squared Error for each label on dev set:\", mse)\n",
        "print(\"Average Mean Squared Error on dev set:\", np.mean(mse))\n",
        "\n",
        "# Load test data and make predictions\n",
        "print(\"Loading test data and predicting...\")\n",
        "X_test = load_unlabeled_data(TEST_FILE)\n",
        "predictions = svm.predict(X_test)\n",
        "\n",
        "# Save predictions to a file\n",
        "output_file = 'test_predictions.txt'\n",
        "print(f\"Saving predictions to {output_file}...\")\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for pred in predictions:\n",
        "        f.write(f\"{pred[0]}\\t{pred[1]}\\n\")\n",
        "print(\"Predictions saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WY2nzuE2OGw",
        "outputId": "080d53a2-29fb-474d-d2c5-52a0aaaaa1c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and loading FastText model...\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz\n",
            "\n",
            "Loading training and development data...\n",
            "Training SVM model...\n",
            "Evaluating on development set...\n",
            "Mean Squared Error for each label on dev set: [0.04801686 0.29895594]\n",
            "Average Mean Squared Error on dev set: 0.1734864019354707\n",
            "Loading test data and predicting...\n",
            "Saving predictions to test_predictions.txt...\n",
            "Predictions saved successfully.\n"
          ]
        }
      ]
    }
  ]
}